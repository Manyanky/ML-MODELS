{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2155,
     "status": "error",
     "timestamp": 1582550437442,
     "user": {
      "displayName": "Benjamin Manyaky",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBYdlXKZP_T_RpwV88V6cMEfLV45F5MovwISPBV=s64",
      "userId": "06644223870603055595"
     },
     "user_tz": -180
    },
    "id": "y2xBVt-8T3GJ",
    "outputId": "9106efac-90a9-4a9a-9b7f-c8fbb3b5641b"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'flower_photos/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-065b51356b58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"shape of data:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"shape of label:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-065b51356b58>\u001b[0m in \u001b[0;36mread_img\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#step4:performing data reading and standardization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mcate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mimgs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'flower_photos/'"
     ]
    }
   ],
   "source": [
    "from skimage import io,transform #scikit-image used for image processing\n",
    "import glob                     #glob for serching a file path comlying to specific rule\n",
    "import os                  #os provide large no of functions to process files and directories\n",
    "import tensorflow as tf     # tf for developing DL algorithms and for constructing CNN\n",
    "import numpy as np         #numpy array used as image object\n",
    "import time                # time processes and converts time\n",
    "\n",
    "#preparing paths\n",
    "path='flower_photos/'\n",
    "model_path='Model/model.ckpt'\n",
    "\n",
    "#step3: preparing standard image parameters.w,h,c are width,lenght and depth of the image\n",
    "w=100\n",
    "h=100\n",
    "c=3\n",
    "\n",
    "#step4:performing data reading and standardization\n",
    "def read_img(path):\n",
    "    cate=[path+x for x in os.listdir(path) if os.path.isdir(path+x)]\n",
    "    imgs=[]\n",
    "    labels=[]\n",
    "    for idx,folder in enumerate(cate):\n",
    "        for im in glob.glob(folder+'/*.jpg'):\n",
    "            print('reading the images:%s'%(im))\n",
    "            img=io.imread(im)\n",
    "            img=transform.resize(img,(w,h))\n",
    "            imgs.append(img)\n",
    "            labels.append(idx)\n",
    "    return np.asarray(imgs,np.float32),np.asarray(labels,np.int32)\n",
    "\n",
    "data,label=read_img(path)\n",
    "print(\"shape of data:\",data.shape)\n",
    "print(\"shape of label:\",label.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 108676,
     "status": "ok",
     "timestamp": 1582550403920,
     "user": {
      "displayName": "Benjamin Manyaky",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBYdlXKZP_T_RpwV88V6cMEfLV45F5MovwISPBV=s64",
      "userId": "06644223870603055595"
     },
     "user_tz": -180
    },
    "id": "cvGmujMCWJA9",
    "outputId": "8620735e-3860-4dc1-b83a-43218b3aaf39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "frcHrBzLT3GV"
   },
   "source": [
    "# Data pre-processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YgeIRKVVT3GX"
   },
   "outputs": [],
   "source": [
    "#step1: Disrupting original data sequence ,use shuffle\n",
    "num_example=data.shape[0]\n",
    "arr=np.arange(num_example)\n",
    "np.random.shuffle(arr)\n",
    "data=data(arr)\n",
    "label=label[arr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qI4U0yJqT3Gc"
   },
   "outputs": [],
   "source": [
    "#step2: setting the training set and verification set\n",
    "ratio=0.8\n",
    "s=np.int(num_example*ratio)\n",
    "x_train=data[:s]\n",
    "y_train=label[:s]\n",
    "x_val=data[s:]\n",
    "y_val=label[s:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B5xPzc-GT3Gh"
   },
   "source": [
    "# Placeholder defining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4AncBaBuT3Gj"
   },
   "outputs": [],
   "source": [
    "#creating placeholder x and y to perform model calculation for the CNN model to transmit tensor data\n",
    "x=tf.placeholder(tf.float32,shape=[None,w,h,c],name='x')\n",
    "y=tf.placeholder(tf.int32,shape=[None,],name='y_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bSqPH00yT3Go"
   },
   "source": [
    "# CNN Construction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "18nLipnjT3Gp"
   },
   "outputs": [],
   "source": [
    "#step1:create a CNN and the hidden layer\n",
    "#define CNN model. 3 varibales,input_tensor,train and regularizer\n",
    "def inference(input_tensor, train, regularizer):\n",
    "    with tf.variable_scope('layer1-conv1'):\n",
    "        conv1_weights=tf.get_variable(\"weight\",[5,5,3,32],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv1_biases=tf.get_variable(\"bias\",[32], initializer=tf.constant_initializer(0.0))\n",
    "        conv1=tf.nn.conv2d(input_tensor, conv1_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu1=tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ePIQ-MhxT3Gu"
   },
   "outputs": [],
   "source": [
    "#Step2: creating second hidden layer\n",
    "with tf.name_scope(\"layer2-pool1\"):\n",
    "    pool1 = tf.nn.max_pool(relu1, ksize = [1,2,2,1],strides=[1,2,2,1],padding=\"VALID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8hCgFLAKT3G0"
   },
   "outputs": [],
   "source": [
    "#step3:createthird hidden layer ie layer1-convo2\n",
    "with tf.variable_scope(\"layer3-convo2\"):\n",
    "    convo2_weigths = tf.get_variable(\"weight\",[5,5,32,64],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    convo2_biases = tf.get_variable(\"bias\",[64], initializer=tf.constant_initializer(0.0))\n",
    "    convo2 = tf.nn.convo2d(pool1, convo2_weights, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    relu2 = tf.nn.relu(tf.nn.bias_add(convo2, conv2_biases))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V9iXQZDKT3G5"
   },
   "outputs": [],
   "source": [
    "#step4:creating the fourth layer\n",
    "with tf.name_scope(\"layer4-pool2\"):\n",
    "    pool2 = tf.nn.max_pool(relu2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding = \"VALID\")\n",
    "    nodes = 25*25*64\n",
    "    reshaped = tf.reshape(pool2, [-1,nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s0UqWNxPT3G-"
   },
   "outputs": [],
   "source": [
    "#step4: creating the  fifth hidden layer\n",
    "with tf.variable_scope(\"layer5-fc1\"):\n",
    "    fc1_weights = tf.get_variable(\"weight\", [nodes, 1024],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    if regularizer != None: tf.add_to_collection('losses', regularizer(fc1_weights))\n",
    "    fc1_biases = tf.get_variable(\"bias\", [1024], initializer=tf.constant_initializer(0.1))\n",
    "    \n",
    "    fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)\n",
    "    if train: fc1 = tf.nn.dropout(fc1, 0.05)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7T2wC4ZpT3HD"
   },
   "outputs": [],
   "source": [
    "#step 6: create the sixth hidden layer\n",
    "with tf.variable_scope('layer6-fc2'):\n",
    "    fc2_weights = tf.get_variable(\"weight\", [1024, 5], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    if regularizer != None: tf.add_to_collection('losses', regularizer(fc2_weights))\n",
    "    fc2_biases = tf.get_variable(\"bias\", [5], initializer=tf.constant_initializer(0.1))\n",
    "    logit = tf.matmul(fc1, fc2_weights) + fc2_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vlW2zaJET3HH"
   },
   "outputs": [],
   "source": [
    "#step7: now we want to view the overarl network structure\n",
    "#entire model has 6 hidden layers, two convolutionary layers,two pooling layers, and two full connection layers\n",
    "\n",
    "def inference (input_tensor, train, regularizer):\n",
    "    with tf.variable_scope('layer1-convo1'):\n",
    "        convo1_weights = tf.get_variable(\"weights\", [5,5,3,32], initializer= tf.truncated_normal_initializer(stddev=0.1))\n",
    "        convo1_biases = tf.get_variable(\"bias\", [32], initializer=tf.constant_initializer(0.0))\n",
    "        convo1 = tf.nn.convo2d(input_tensor, convo1_weights,strides=[1, 1, 1, 1], paddding = 'SAME')\n",
    "        relu1 = tf.nn.relu(tf.nn.bias_add(convo1, convo1_biases))\n",
    "        \n",
    "    with tf.namescope(\"layer2-pool1\"):\n",
    "        pool1 = tf.nn.max_pool(relu1, ksize= [1,2,2,1], strides=[1,2,2,1], padding=\"VALID\")\n",
    "    \n",
    "    with tf.variable_scope(\"layer3-convo2\"):\n",
    "        convo2_weights = tf.get_varible(\"weight\",[5,5,32,64],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        convo2_biases = tf.get_variable(\"bias\", [64], initializer=tf.constant_initializer(0.0))\n",
    "        convo2 = tf.nn.convo2d(pool1, convo2_weights,strides=[1, 1, 1, 1], paddding = 'SAME')\n",
    "        relu2 = tf.nn.relu(tf.nn.bias_add(convo2, convo2_biases))\n",
    "    \n",
    "    with tf.name_scope(\"layer4-pool2\"):\n",
    "        pool2 = tf.nn.max_pool(relu2, ksize= [1,2,2,1], strides=[1,2,2,1], padding=\"VALID\")\n",
    "        nodes = 25*25*64\n",
    "        reshaped = tf.reshape(pool2,[-1,nodes])\n",
    "    \n",
    "    with tf.variable_scope('layer5-fc1'):\n",
    "        fc1_weights = tf.get_variable(\"weight\", [nodes,1024], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer != None: tf.add_to_collection('losses', regularizer(fc1_weights))\n",
    "        fc1_biases = tf.get_variable(\"bias\", [1024], initializer=tf.constant_initializer(0.1))\n",
    "        \n",
    "        fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)\n",
    "        tf train: fc1 = tf.nn.dropout(fc1, 0.5)\n",
    "    \n",
    "     with tf.variable_scope('layer6-fc2'):\n",
    "        fc2_weights = tf.get_variable(\"weight\", [1024, 5], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer != None: tf.add_to_collection('losses', regularizer(fc2_weights))\n",
    "        fc2_biases = tf.get_variable(\"bias\", [5], initializer=tf.constant_initializer(0.1))\n",
    "        logit = tf.matmul(fc1, fc2_weights) + fc2_biases\n",
    "    \n",
    "    return logit\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sl7pXx5DT3HK"
   },
   "outputs": [],
   "source": [
    "#step8: defining\n",
    "# the document mainly uses L2  regular terms to prevent  overfitting and improve model generalization capabilities\n",
    "\n",
    "regularizer = tf.contrib.layers.12_regularizer(0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "He4kLb5CT3HO"
   },
   "outputs": [],
   "source": [
    "#step 9 : viewing the model, to prepare subsequent modeltest\n",
    "\n",
    "logits = model(x, False, regularizer)\n",
    "print(\"shape of logits:\",logits.shape)\n",
    "\n",
    "b = tf.constant(value=1, dtype=tf.float32)\n",
    "logits_eval = tf.multiply(logits,b,name='logits_eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9gBBP24ZT3HS"
   },
   "source": [
    "# Loss function, optimizer, and verification indicator defining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7cNhE007T3HT"
   },
   "outputs": [],
   "source": [
    "#step1: defining the loss function\n",
    "#calculating the sparse cross entropy between the prediction tag logits and original tag y_\n",
    "\n",
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pHfZuqTZT3HZ"
   },
   "outputs": [],
   "source": [
    "#step2:defining optimizer, using adam optimizer\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(Learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "28tcsdgST3Hd"
   },
   "outputs": [],
   "source": [
    "#step3: defining optimization target, training operation and minimizing the loss\n",
    "\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jnd7RoC-T3Hh"
   },
   "outputs": [],
   "source": [
    "#step4: defining accuracy of verification indicators.accuracay function for model effect verification\n",
    "\n",
    "correct_prediction = tf.equal(tf.cast(tf.argmax(logits,1), tf.int32), y_)\n",
    "acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U4oI-kwUT3Hm"
   },
   "source": [
    "# Model Training and Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VTOtcs2sT3Hn"
   },
   "outputs": [],
   "source": [
    "#step1: defining the function for extracting data in batches\n",
    "\n",
    "def minibatches(inputs=None, targets=None, batch_size=None, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batch_size + 1, batch_size):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batch_size]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batch_size)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "89yo0ubqT3Hs"
   },
   "outputs": [],
   "source": [
    "#step 2 :prepare training parameters and perform model initialization\n",
    "\n",
    "n_epoch = 10\n",
    "batch_size = 64\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w5Vzdt3ET3Hw"
   },
   "outputs": [],
   "source": [
    "#step3: defining model operation on the training set and verification set and print the results\n",
    "#training a total of 5 epochs, each epoch need to run on the training set and verification set\n",
    "#print the corresponding loss value\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    print(\"epooch\", epoch+1)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #training\n",
    "    train_loss, train_acc, n_batch = 0, 0, 0\n",
    "    for x_train_a, y_train_a in minibatches(x_train, y_train, batch_size, shuffle=True):\n",
    "        _,err,ac=sess.run([train_op,loss,acc], feed_dict={x: x_train_a, y_:y_train_a})\n",
    "        train_loss += err; train_acc += ac; n_batch +=1\n",
    "    print(\" train loss: %f\" % (np.sum(train_loss)/n_batch))\n",
    "    print(\" train acc: %f\" % (np.sum(train_acc)/n_batch))\n",
    "    \n",
    "    #validation\n",
    "    val_loss, val_acc, n_batch = 0, 0, 0\n",
    "    for x_val_a, y_val_a in minibatches(x_train, y_train, batch_size, shuffle=False):\n",
    "        err, ac = sess.run([train_op, loss, acc], feed_dict={x: x_train_a, y_:y_train_a})\n",
    "        val_loss += err; val_acc += ac; n_batch += 1\n",
    "    print(\" validation loss: %f\" % (np.sum(val_loss)/n_batch))\n",
    "    print(\" validation acc: %f\" % (np.sum(val_acc)/n_batch))\n",
    "    print(\" epoch time: %f\" % (time.time() - start_time)\n",
    "    print('--------------------------------------------------------')\n",
    "\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "we7y3eSZT3H0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HsnxO4WvT3H5"
   },
   "source": [
    "# Model saving and session closing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dkDc5JWfT3H6"
   },
   "outputs": [],
   "source": [
    "#step1: saving the model,\n",
    "\n",
    "saver.save(sess,model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qPzX8kYsT3H9"
   },
   "outputs": [],
   "source": [
    "#step2: closing the session\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WGJoyOBjT3IB"
   },
   "source": [
    "# Sample image testing, invoking the Model for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_w6tLk7pT3IC"
   },
   "outputs": [],
   "source": [
    "#step1: import modules\n",
    "path1 = \"flower_photos/daisy/5547758_eea9edfd54_n.jpg\"\n",
    "path2 = \"flower_photos/dandelion/735522_b66e5d3078_m.jpg\"\n",
    "path3 = \"flower_photos/roses/394990940_7af082cf8d_n.jpg\"\n",
    "path4 = \"flower_photos/sunflowers/6953297_8576bf4ea3.jpg\"\n",
    "path5 = \"flower_photos/tulips/10791227_7168491604.jpg\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jEBGpE70T3IH"
   },
   "outputs": [],
   "source": [
    "#step2: generate a type dictionary, for each flower type for type matching\n",
    "flower_dict = {0:'daisy',1:'dandelion',2:'roses',3:'sunflowers',4:'tulips'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oqS97OEST3IK"
   },
   "outputs": [],
   "source": [
    "#step3: defining image starndardization function\n",
    "w=100\n",
    "h=100\n",
    "c=3\n",
    "\n",
    "def read_one_image(path):\n",
    "    img = io.imread(path)\n",
    "    img = transform.resize(img, (w,h))\n",
    "    return np.asarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KsJj5KRwT3IP"
   },
   "outputs": [],
   "source": [
    "#step 4: standard the test data, invoke the read one image\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    data = []\n",
    "    data1 = read_one_image(path1)\n",
    "    data2 = read_one_image(path2)\n",
    "    data3 = read_one_image(path3)\n",
    "    data4 = read_one_image(path4)\n",
    "    data5 = read_one_image(path5)\n",
    "    data.append(data1)\n",
    "    data.append(data2)\n",
    "    data.append(data3)\n",
    "    data.append(data4)\n",
    "    data.append(data5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NGfjhYW_T3IT"
   },
   "outputs": [],
   "source": [
    "#step5: Reload the model\n",
    "saver = tf.train.import_meta_graph('Model/model.ckpt.meta')\n",
    "saver.restore(sess,tf.train.latest_checkpoint('Model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eq6CbzsFT3IZ"
   },
   "outputs": [],
   "source": [
    "#step 6: output model parameters\n",
    "graph = tf.get_default_graph()\n",
    "x = graph.get_tensor_by_name(\"x:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rhW8ZTnhT3Ie"
   },
   "outputs": [],
   "source": [
    "#step 7: invoking the model\n",
    "feed_dict = {x:data}\n",
    "\n",
    "logits = graph.get_tensor_by_name(\"logits_eval:0\")\n",
    "\n",
    "classifcation_result = sess.run(logits, feed_dict)\n",
    "\n",
    "print(classification_result)\n",
    "print(tf.argmax(classification_result,1).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DmknU80uT3Ii",
    "outputId": "6b223c93-f852-4ca5-e9ad-79bcfb5814a9"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-021a2fd1054a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#step 8: print the test resulst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"flower\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"prediction:\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mflower_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "#step 8: print the test resulst\n",
    "output = []\n",
    "output = tf.argmax(classification_result, 1).eval()\n",
    "for i in range(len(output)):\n",
    "    print(\"flower\", i+1,\"prediction:\"+flower_dict[output[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0vJvehebT3Im"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Image-Recog-FLOWERS.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
